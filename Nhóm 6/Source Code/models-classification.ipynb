{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7989628,"sourceType":"datasetVersion","datasetId":4703395},{"sourceId":169573207,"sourceType":"kernelVersion"},{"sourceId":169837626,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\ndirectory = '/kaggle/input/learntools/'\nfor filename in os.listdir(directory):\n    print(os.path.join(directory, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-02T01:43:28.342825Z","iopub.execute_input":"2024-04-02T01:43:28.343073Z","iopub.status.idle":"2024-04-02T01:43:28.361754Z","shell.execute_reply.started":"2024-04-02T01:43:28.34305Z","shell.execute_reply":"2024-04-02T01:43:28.360931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. MLP","metadata":{}},{"cell_type":"markdown","source":"**Tiền xử lý dữ liệu**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.utils import to_categorical\n\n# Function to read data from CSV file\ndef read_data(pathCSV):\n    filenames = []\n    labels = []\n    \n    df = pd.read_csv(pathCSV)\n    \n    for index, row in df.iterrows():\n        filename = row['filename']\n        class_name = row['class']\n        \n        filenames.append(filename)\n        labels.append(class_name)\n        \n    return filenames, labels\n\n# Directories for train, valid, test data\ntrain_data_dir = '/kaggle/input/learntools/train/'\nvalidation_data_dir = '/kaggle/input/learntools/valid/'\ntest_data_dir = '/kaggle/input/learntools/test/'\n\n# Read data from train, valid, and test sets\ntrain_filenames, train_labels = read_data(os.path.join(train_data_dir, \"_annotations.csv\"))\ntest_filenames, test_labels = read_data(os.path.join(test_data_dir, \"_annotations.csv\"))\nvalidation_filenames, validation_labels = read_data(os.path.join(validation_data_dir, \"_annotations.csv\"))\n\n\n# Xử lý dữ liệu hình ảnh từ tập train\ntrain_images = []\ntrain_labels_list = []\nfor filename, label in zip(train_filenames, train_labels):\n    image = cv2.imread(os.path.join(train_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    train_images.append(image)\n    train_labels_list.append(label)\n\ntrain_images = np.array(train_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\ntrain_labels = pd.get_dummies(train_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n\n# Xử lý dữ liệu hình ảnh từ tập valid\nvalid_images = []\nvalid_labels_list = []\nfor filename, label in zip(validation_filenames, validation_labels):\n    image = cv2.imread(os.path.join(validation_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    valid_images.append(image)\n    valid_labels_list.append(label)\n\nvalid_images = np.array(valid_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\nvalid_labels = pd.get_dummies(valid_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n\n# Xử lý dữ liệu hình ảnh từ tập test\ntest_images = []\ntest_labels_list = []\nfor filename, label in zip(test_filenames, test_labels):\n    image = cv2.imread(os.path.join(test_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    test_images.append(image)\n    test_labels_list.append(label)\n\ntest_images = np.array(test_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\ntest_labels = pd.get_dummies(test_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:43:28.40965Z","iopub.execute_input":"2024-04-02T01:43:28.409905Z","iopub.status.idle":"2024-04-02T01:44:12.276624Z","shell.execute_reply.started":"2024-04-02T01:43:28.409882Z","shell.execute_reply":"2024-04-02T01:44:12.275541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\n\ndef create_mlp_model():\n    model = Sequential()\n\n    # Flatten input layer\n    model.add(Flatten(input_shape=(224, 224, 3)))  # Đầu vào có kích thước (224, 224, 3)\n\n    # Fully connected layers\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))  # Dropout để tránh overfitting\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n\n    # Output layer\n    model.add(Dense(5, activation='softmax'))\n\n    return model\n\n# Tạo mô hình MLP\nmodel_mlp = create_mlp_model()\n\n# Hiển thị summary của mô hình để kiểm tra số lượng tham số\nmodel_mlp.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:44:12.278308Z","iopub.execute_input":"2024-04-02T01:44:12.278589Z","iopub.status.idle":"2024-04-02T01:44:13.222763Z","shell.execute_reply.started":"2024-04-02T01:44:12.278566Z","shell.execute_reply":"2024-04-02T01:44:13.221871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Biên dịch mô hình\nmodel_mlp.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Huấn luyện mô hình\nhistory_mlp = model_mlp.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(valid_images, valid_labels))\n\n# Lưu mô hình\nmodel_mlp.save('/kaggle/working/classification_MLP_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:44:13.223807Z","iopub.execute_input":"2024-04-02T01:44:13.224105Z","iopub.status.idle":"2024-04-02T01:44:56.39385Z","shell.execute_reply.started":"2024-04-02T01:44:13.224061Z","shell.execute_reply":"2024-04-02T01:44:56.393057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Biểu đồ loss và accuracy\nplt.plot(history_mlp.history['loss'], label='Training Loss')\nplt.plot(history_mlp.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(history_mlp.history['accuracy'], label='Training Accuracy')\nplt.plot(history_mlp.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:44:56.396572Z","iopub.execute_input":"2024-04-02T01:44:56.396929Z","iopub.status.idle":"2024-04-02T01:44:56.907094Z","shell.execute_reply.started":"2024-04-02T01:44:56.396897Z","shell.execute_reply":"2024-04-02T01:44:56.90616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Đánh giá mô hình trên tập test\ntest_loss_mlp, test_accuracy_mlp = model_mlp.evaluate(test_images, test_labels)\nprint(\"Test Loss:\", test_loss_mlp)\nprint(\"Test Accuracy:\", test_accuracy_mlp)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:44:56.908321Z","iopub.execute_input":"2024-04-02T01:44:56.908594Z","iopub.status.idle":"2024-04-02T01:44:57.873414Z","shell.execute_reply.started":"2024-04-02T01:44:56.908571Z","shell.execute_reply":"2024-04-02T01:44:57.872492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\n# Xây dựng mô hình MLP cải tiến\ndef create_improved_mlp_model(input_shape, num_classes):\n    model = Sequential([\n        Dense(512, activation='relu', input_shape=input_shape),\n        Dropout(0.5),  # Dropout để tránh overfitting\n        Dense(256, activation='relu'),\n        Dropout(0.3),  \n        Dense(128, activation='relu'),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model\n\n# Tạo mô hình\nmodel_improved_mlp = create_improved_mlp_model(input_shape=(224*224*3,), num_classes=5)\n\n# Biên dịch mô hình\nmodel_improved_mlp.compile(optimizer=Adam(learning_rate=0.001),\n                            loss='categorical_crossentropy',\n                            metrics=['accuracy'])\n# Huấn luyện mô hình\nhistory_improved_mlp = model_improved_mlp.fit(train_images.reshape(-1, 224*224*3), \n                                              train_labels, \n                                              epochs=100, \n                                              batch_size=32, \n                                              validation_data=(valid_images.reshape(-1, 224*224*3), valid_labels))\n# Biểu đồ loss và accuracy\nplt.plot(history_improved_mlp.history['loss'], label='Training Loss')\nplt.plot(history_improved_mlp.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(history_improved_mlp.history['accuracy'], label='Training Accuracy')\nplt.plot(history_improved_mlp.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:44:57.874543Z","iopub.execute_input":"2024-04-02T01:44:57.874814Z","iopub.status.idle":"2024-04-02T01:49:00.209098Z","shell.execute_reply.started":"2024-04-02T01:44:57.87479Z","shell.execute_reply":"2024-04-02T01:49:00.208265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Đánh giá mô hình trên tập test\ntest_loss_improved_mlp, test_accuracy_improved_mlp = model_improved_mlp.evaluate(test_images.reshape(-1, 224*224*3), test_labels)\nprint(\"Test Loss:\", test_loss_improved_mlp)\nprint(\"Test Accuracy:\", test_accuracy_improved_mlp)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:49:00.210462Z","iopub.execute_input":"2024-04-02T01:49:00.21081Z","iopub.status.idle":"2024-04-02T01:49:00.781526Z","shell.execute_reply.started":"2024-04-02T01:49:00.210777Z","shell.execute_reply":"2024-04-02T01:49:00.780673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lưu mô hình\nmodel_improved_mlp.save('/kaggle/working/classification_MLP_model_v2.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:49:00.782731Z","iopub.execute_input":"2024-04-02T01:49:00.783018Z","iopub.status.idle":"2024-04-02T01:49:02.566714Z","shell.execute_reply.started":"2024-04-02T01:49:00.782993Z","shell.execute_reply":"2024-04-02T01:49:02.565885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. CNN","metadata":{}},{"cell_type":"markdown","source":"**Tiền xử lý dữ liêu**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.utils import to_categorical\n\n# Function to read data from CSV file\ndef read_data(pathCSV):\n    filenames = []\n    labels = []\n    \n    df = pd.read_csv(pathCSV)\n    \n    for index, row in df.iterrows():\n        filename = row['filename']\n        class_name = row['class']\n        \n        filenames.append(filename)\n        labels.append(class_name)\n        \n    return filenames, labels\n\n# Directories for train, valid, test data\ntrain_data_dir = '/kaggle/input/learntools/train/'\nvalidation_data_dir = '/kaggle/input/learntools/valid/'\ntest_data_dir = '/kaggle/input/learntools/test/'\n\n# Read data from train, valid, and test sets\ntrain_filenames, train_labels = read_data(os.path.join(train_data_dir, \"_annotations.csv\"))\ntest_filenames, test_labels = read_data(os.path.join(test_data_dir, \"_annotations.csv\"))\nvalidation_filenames, validation_labels = read_data(os.path.join(validation_data_dir, \"_annotations.csv\"))\n\n\n# Xử lý dữ liệu hình ảnh từ tập train\ntrain_images = []\ntrain_labels_list = []\nfor filename, label in zip(train_filenames, train_labels):\n    image = cv2.imread(os.path.join(train_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    train_images.append(image)\n    train_labels_list.append(label)\n\ntrain_images = np.array(train_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\ntrain_labels = pd.get_dummies(train_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n\n# Xử lý dữ liệu hình ảnh từ tập valid\nvalid_images = []\nvalid_labels_list = []\nfor filename, label in zip(validation_filenames, validation_labels):\n    image = cv2.imread(os.path.join(validation_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    valid_images.append(image)\n    valid_labels_list.append(label)\n\nvalid_images = np.array(valid_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\nvalid_labels = pd.get_dummies(valid_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n\n# Xử lý dữ liệu hình ảnh từ tập test\ntest_images = []\ntest_labels_list = []\nfor filename, label in zip(test_filenames, test_labels):\n    image = cv2.imread(os.path.join(test_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    test_images.append(image)\n    test_labels_list.append(label)\n\ntest_images = np.array(test_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\ntest_labels = pd.get_dummies(test_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:49:02.568057Z","iopub.execute_input":"2024-04-02T01:49:02.568495Z","iopub.status.idle":"2024-04-02T01:49:08.81998Z","shell.execute_reply.started":"2024-04-02T01:49:02.56846Z","shell.execute_reply":"2024-04-02T01:49:08.818899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\ndef create_simple_cnn_model(input_shape, num_classes):\n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n        MaxPooling2D((2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D((2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        Flatten(),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model\n\n# Tạo mô hình\nmodel_simple_cnn = create_simple_cnn_model(input_shape=(224, 224, 3), num_classes=5)\n\n# Biên dịch mô hình\nmodel_simple_cnn.compile(optimizer='adam',\n                          loss='categorical_crossentropy',\n                          metrics=['accuracy'])\n\n# In tổng quan của mô hình\nmodel_simple_cnn.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:49:08.828592Z","iopub.execute_input":"2024-04-02T01:49:08.828894Z","iopub.status.idle":"2024-04-02T01:49:08.909133Z","shell.execute_reply.started":"2024-04-02T01:49:08.828868Z","shell.execute_reply":"2024-04-02T01:49:08.908272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Huấn luyện mô hình\nhistory_simple_cnn = model_simple_cnn.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(valid_images, valid_labels))\n\n# Biểu đồ loss và accuracy\nimport matplotlib.pyplot as plt\n\nplt.plot(history_simple_cnn.history['loss'], label='Training Loss')\nplt.plot(history_simple_cnn.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(history_simple_cnn.history['accuracy'], label='Training Accuracy')\nplt.plot(history_simple_cnn.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Đánh giá mô hình trên tập test\ntest_loss_simple_cnn, test_accuracy_simple_cnn = model_simple_cnn.evaluate(test_images, test_labels)\nprint(\"Test Loss:\", test_loss_simple_cnn)\nprint(\"Test Accuracy:\", test_accuracy_simple_cnn)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:49:08.910349Z","iopub.execute_input":"2024-04-02T01:49:08.910683Z","iopub.status.idle":"2024-04-02T01:50:07.041598Z","shell.execute_reply.started":"2024-04-02T01:49:08.910652Z","shell.execute_reply":"2024-04-02T01:50:07.040667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lưu mô hình\nmodel_simple_cnn.save('/kaggle/working/classification_CNN_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:50:07.042987Z","iopub.execute_input":"2024-04-02T01:50:07.04339Z","iopub.status.idle":"2024-04-02T01:50:07.331498Z","shell.execute_reply.started":"2024-04-02T01:50:07.043355Z","shell.execute_reply":"2024-04-02T01:50:07.330631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_improved_cnn_model(input_shape, num_classes):\n    model = Sequential([\n        Conv2D(64, (3, 3), activation='relu', input_shape=input_shape),\n        MaxPooling2D((2, 2)),\n        Conv2D(128, (3, 3), activation='relu'),\n        MaxPooling2D((2, 2)),\n        Conv2D(256, (3, 3), activation='relu'),\n        MaxPooling2D((2, 2)),\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    return model\n\n# Tạo mô hình cải thiện\nmodel_improved_cnn = create_improved_cnn_model(input_shape=(224, 224, 3), num_classes=5)\n\n# Biên dịch mô hình\nmodel_improved_cnn.compile(optimizer='adam',\n                            loss='categorical_crossentropy',\n                            metrics=['accuracy'])\n\n# Huấn luyện mô hình\nhistory_improved_cnn = model_improved_cnn.fit(train_images, train_labels, epochs=100, batch_size=32, validation_data=(valid_images, valid_labels))\n\n# Đánh giá mô hình trên tập test\ntest_loss_improved_cnn, test_accuracy_improved_cnn = model_improved_cnn.evaluate(test_images, test_labels)\nprint(\"Test Loss:\", test_loss_improved_cnn)\nprint(\"Test Accuracy:\", test_accuracy_improved_cnn)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T01:50:07.332652Z","iopub.execute_input":"2024-04-02T01:50:07.332952Z","iopub.status.idle":"2024-04-02T02:05:39.337538Z","shell.execute_reply.started":"2024-04-02T01:50:07.332926Z","shell.execute_reply":"2024-04-02T02:05:39.336659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Biểu đồ loss và accuracy\nimport matplotlib.pyplot as plt\n\nplt.plot(history_improved_cnn.history['loss'], label='Training Loss')\nplt.plot(history_improved_cnn.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(history_improved_cnn.history['accuracy'], label='Training Accuracy')\nplt.plot(history_improved_cnn.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:05:39.338844Z","iopub.execute_input":"2024-04-02T02:05:39.339174Z","iopub.status.idle":"2024-04-02T02:05:39.826729Z","shell.execute_reply.started":"2024-04-02T02:05:39.339147Z","shell.execute_reply":"2024-04-02T02:05:39.82579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lưu mô hình\nmodel_improved_cnn.save('/kaggle/working/classification_CNN_model_v2.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:05:39.83332Z","iopub.execute_input":"2024-04-02T02:05:39.833632Z","iopub.status.idle":"2024-04-02T02:05:41.918726Z","shell.execute_reply.started":"2024-04-02T02:05:39.833604Z","shell.execute_reply":"2024-04-02T02:05:41.917926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. VGG","metadata":{}},{"cell_type":"markdown","source":"**Tiền xử lý dữ liệu**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.utils import to_categorical\n\n# Function to read data from CSV file\ndef read_data(pathCSV):\n    filenames = []\n    labels = []\n    \n    df = pd.read_csv(pathCSV)\n    \n    for index, row in df.iterrows():\n        filename = row['filename']\n        class_name = row['class']\n        \n        filenames.append(filename)\n        labels.append(class_name)\n        \n    return filenames, labels\n\n# Directories for train, valid, test data\ntrain_data_dir = '/kaggle/input/learntools/train/'\nvalidation_data_dir = '/kaggle/input/learntools/valid/'\ntest_data_dir = '/kaggle/input/learntools/test/'\n\n# Read data from train, valid, and test sets\ntrain_filenames, train_labels = read_data(os.path.join(train_data_dir, \"_annotations.csv\"))\ntest_filenames, test_labels = read_data(os.path.join(test_data_dir, \"_annotations.csv\"))\nvalidation_filenames, validation_labels = read_data(os.path.join(validation_data_dir, \"_annotations.csv\"))\n\n\n# Xử lý dữ liệu hình ảnh từ tập train\ntrain_images = []\ntrain_labels_list = []\nfor filename, label in zip(train_filenames, train_labels):\n    image = cv2.imread(os.path.join(train_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    train_images.append(image)\n    train_labels_list.append(label)\n\ntrain_images = np.array(train_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\ntrain_labels = pd.get_dummies(train_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n\n# Xử lý dữ liệu hình ảnh từ tập valid\nvalid_images = []\nvalid_labels_list = []\nfor filename, label in zip(validation_filenames, validation_labels):\n    image = cv2.imread(os.path.join(validation_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    valid_images.append(image)\n    valid_labels_list.append(label)\n\nvalid_images = np.array(valid_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\nvalid_labels = pd.get_dummies(valid_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n\n# Xử lý dữ liệu hình ảnh từ tập test\ntest_images = []\ntest_labels_list = []\nfor filename, label in zip(test_filenames, test_labels):\n    image = cv2.imread(os.path.join(test_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    test_images.append(image)\n    test_labels_list.append(label)\n\ntest_images = np.array(test_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\ntest_labels = pd.get_dummies(test_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:05:41.919819Z","iopub.execute_input":"2024-04-02T02:05:41.920067Z","iopub.status.idle":"2024-04-02T02:05:47.688849Z","shell.execute_reply.started":"2024-04-02T02:05:41.920045Z","shell.execute_reply":"2024-04-02T02:05:47.68797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Xây dựng mô hình VGG16**","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport os\n\ndef VGG16(input_shape, num_classes):\n    model = Sequential()\n\n    # Khối VGG 1\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape, name='conv2d'))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_1'))\n    model.add(MaxPooling2D(pool_size=2, strides=2, name='max_pooling2d'))\n\n    # Khối VGG 2\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_2'))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_3'))\n    model.add(MaxPooling2D(pool_size=2, strides=2, name='max_pooling2d_1'))\n\n    # Khối VGG 3\n    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_4'))\n    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_5'))\n    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_6'))\n    model.add(MaxPooling2D(pool_size=2, strides=2, name='max_pooling2d_2'))\n\n    # Khối VGG 4\n    model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_7'))\n    model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_8'))\n    model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_9'))\n    model.add(MaxPooling2D(pool_size=2, strides=2, name='max_pooling2d_3'))\n\n    # Khối VGG 5\n    model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_10'))\n    model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_11'))\n    model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', name='conv2d_12'))\n    model.add(MaxPooling2D(pool_size=2, strides=2, name='max_pooling2d_4'))\n\n    model.add(Flatten(name='flatten'))\n\n    # Fully connected layer\n    model.add(Dense(4096, activation='relu', name='dense'))\n    model.add(Dropout(0.5, name='dropout'))\n\n    model.add(Dense(4096, activation='relu', name='dense_1'))\n    model.add(Dropout(0.5, name='dropout_1'))\n\n    model.add(Dense(num_classes, activation='softmax', name='dense_2'))\n\n    return model\n\n# Dữ liệu huấn luyện, validation, và kiểm thử đã được tiền xử lý và chuẩn bị trước\n\n# Kích thước của ảnh đầu vào\ninput_shape = (224, 224, 3)\n\n# Số lượng lớp đầu ra (số lượng lớp trong tập dữ liệu)\nnum_classes = 5\n\n# Tạo mô hình VGG16\nmodel_vgg16 = VGG16(input_shape, num_classes)\n\n# Biên dịch mô hình\nmodel_vgg16.compile(optimizer='adam',\n                     loss='categorical_crossentropy',\n                     metrics=['accuracy'])\n\n# Hiển thị tổng quan của mô hình\nmodel_vgg16.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:05:47.69024Z","iopub.execute_input":"2024-04-02T02:05:47.690637Z","iopub.status.idle":"2024-04-02T02:05:48.061627Z","shell.execute_reply.started":"2024-04-02T02:05:47.690598Z","shell.execute_reply":"2024-04-02T02:05:48.060768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Huấn luyện mô hình\nhistory_vgg16 = model_vgg16.fit(train_images, train_labels, epochs=20, batch_size=32, validation_data=(valid_images, valid_labels))","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:05:48.062793Z","iopub.execute_input":"2024-04-02T02:05:48.06307Z","iopub.status.idle":"2024-04-02T02:19:43.888862Z","shell.execute_reply.started":"2024-04-02T02:05:48.063047Z","shell.execute_reply":"2024-04-02T02:19:43.887869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Biểu đồ loss và accuracy\nimport matplotlib.pyplot as plt\n\nplt.plot(history_vgg16.history['loss'], label='Training Loss')\nplt.plot(history_vgg16.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.plot(history_vgg16.history['accuracy'], label='Training Accuracy')\nplt.plot(history_vgg16.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Đánh giá mô hình trên tập test\ntest_loss_vgg16, test_accuracy_vgg16 = model_vgg16.evaluate(test_images, test_labels)\nprint(\"Test Loss:\", test_loss_vgg16)\nprint(\"Test Accuracy:\", test_accuracy_vgg16)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:19:43.890679Z","iopub.execute_input":"2024-04-02T02:19:43.891588Z","iopub.status.idle":"2024-04-02T02:19:51.17253Z","shell.execute_reply.started":"2024-04-02T02:19:43.89155Z","shell.execute_reply":"2024-04-02T02:19:51.171618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lưu mô hình\nmodel_vgg16.save('/kaggle/working/classification_VGG16_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:19:51.17846Z","iopub.execute_input":"2024-04-02T02:19:51.178761Z","iopub.status.idle":"2024-04-02T02:19:54.183865Z","shell.execute_reply.started":"2024-04-02T02:19:51.178736Z","shell.execute_reply":"2024-04-02T02:19:54.183007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LeNet","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.utils import to_categorical\n\n# Function to read data from CSV file\ndef read_data(pathCSV):\n    filenames = []\n    labels = []\n    \n    df = pd.read_csv(pathCSV)\n    \n    for index, row in df.iterrows():\n        filename = row['filename']\n        class_name = row['class']\n        \n        filenames.append(filename)\n        labels.append(class_name)\n        \n    return filenames, labels\n\n# Directories for train, valid, test data\ntrain_data_dir = '/kaggle/input/learntools/train/'\nvalidation_data_dir = '/kaggle/input/learntools/valid/'\ntest_data_dir = '/kaggle/input/learntools/test/'\n\n# Read data from train, valid, and test sets\ntrain_filenames, train_labels = read_data(os.path.join(train_data_dir, \"_annotations.csv\"))\ntest_filenames, test_labels = read_data(os.path.join(test_data_dir, \"_annotations.csv\"))\nvalidation_filenames, validation_labels = read_data(os.path.join(validation_data_dir, \"_annotations.csv\"))\n\n\n# Xử lý dữ liệu hình ảnh từ tập train\ntrain_images = []\ntrain_labels_list = []\nfor filename, label in zip(train_filenames, train_labels):\n    image = cv2.imread(os.path.join(train_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    train_images.append(image)\n    train_labels_list.append(label)\n\ntrain_images = np.array(train_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\ntrain_labels = pd.get_dummies(train_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n\n# Xử lý dữ liệu hình ảnh từ tập valid\nvalid_images = []\nvalid_labels_list = []\nfor filename, label in zip(validation_filenames, validation_labels):\n    image = cv2.imread(os.path.join(validation_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    valid_images.append(image)\n    valid_labels_list.append(label)\n\nvalid_images = np.array(valid_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\nvalid_labels = pd.get_dummies(valid_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n\n# Xử lý dữ liệu hình ảnh từ tập test\ntest_images = []\ntest_labels_list = []\nfor filename, label in zip(test_filenames, test_labels):\n    image = cv2.imread(os.path.join(test_data_dir, filename))\n    image = cv2.resize(image, (224, 224))  # Resize hình ảnh về kích thước 224x224\n    test_images.append(image)\n    test_labels_list.append(label)\n\ntest_images = np.array(test_images) / 255.0  # Chuẩn hóa giá trị pixel về khoảng [0, 1]\ntest_labels = pd.get_dummies(test_labels_list)  # Chuyển đổi nhãn sang one-hot encoding\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:19:54.18516Z","iopub.execute_input":"2024-04-02T02:19:54.18545Z","iopub.status.idle":"2024-04-02T02:19:59.971327Z","shell.execute_reply.started":"2024-04-02T02:19:54.185426Z","shell.execute_reply":"2024-04-02T02:19:59.970013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n\ndef LeNet5(input_shape=(224, 224, 3), num_classes=5):\n    model = Sequential()\n\n    # C1 Convolutional layer\n    model.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=input_shape, padding='same', name='C1'))\n\n    # S2 Sub-sampling layer (pooling layer)\n    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), name='S2'))\n\n    # C3 Convolutional layer\n    model.add(Conv2D(16, kernel_size=(5, 5), activation='tanh', padding='valid', name='C3'))\n\n    # S4 Sub-sampling layer (pooling layer)\n    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), name='S4'))\n\n    # C5 Convolutional layer\n    model.add(Conv2D(120, kernel_size=(5, 5), activation='tanh', padding='valid', name='C5'))\n\n    model.add(Flatten(name='Flatten'))\n\n    # FC6 Fully connected\n    model.add(Dense(84, activation='tanh', name='FC6'))\n\n    # FC7 output\n    model.add(Dense(num_classes, activation='softmax', name='FC7'))\n    return model\n\nif __name__ == '__main__':\n    model_lenet = LeNet5()\n    print(model_lenet.summary())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:19:59.972787Z","iopub.execute_input":"2024-04-02T02:19:59.973104Z","iopub.status.idle":"2024-04-02T02:20:00.081705Z","shell.execute_reply.started":"2024-04-02T02:19:59.973065Z","shell.execute_reply":"2024-04-02T02:20:00.08078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nimport os\n\n# Huấn luyện mô hình\ndef train_model(model, train_images, train_labels, valid_images, valid_labels, batch_size=32, epochs=10):\n    # Compile model\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Define callbacks\n    checkpoint = ModelCheckpoint('lenet_model.keras', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n    # Train the model\n    history = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs,\n                        validation_data=(valid_images, valid_labels), callbacks=[checkpoint, early_stopping])\n\n    return history\n\n# Hiển thị biểu đồ đánh giá\ndef plot_evaluation(history):\n    plt.plot(history.history['accuracy'], label='accuracy')\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n# Load preprocessed data\n# Replace train_images, train_labels, valid_images, valid_labels with your preprocessed data\n\n# Train the model\nhistory = train_model(model_lenet, train_images, train_labels, valid_images, valid_labels, batch_size=32, epochs=10)\n\n# Plot evaluation\nplot_evaluation(history)\n\n# Evaluate on test set\ntest_loss_lenet, test_accuracy_lenet = model_lenet.evaluate(test_images, test_labels)\nprint(\"Test Loss:\", test_loss_lenet)\nprint(\"Test Accuracy:\", test_accuracy_lenet)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:20:00.083077Z","iopub.execute_input":"2024-04-02T02:20:00.083692Z","iopub.status.idle":"2024-04-02T02:20:37.169238Z","shell.execute_reply.started":"2024-04-02T02:20:00.083659Z","shell.execute_reply":"2024-04-02T02:20:37.168278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lưu mô hình\nmodel_lenet.save('/kaggle/working/lenet_model.keras')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:20:37.173959Z","iopub.execute_input":"2024-04-02T02:20:37.17432Z","iopub.status.idle":"2024-04-02T02:20:38.480127Z","shell.execute_reply.started":"2024-04-02T02:20:37.174293Z","shell.execute_reply":"2024-04-02T02:20:38.479249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, BatchNormalization\n\ndef Improved_LeNet():\n    model = Sequential()\n\n    # C1 Convolutional layer\n    model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=(224, 224, 3), padding='same', name='C1'))\n    model.add(BatchNormalization())\n    # S2 Sub-sampling layer (pooling layer)\n    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), name='S2'))\n\n    # C3 Convolutional layer\n    model.add(Conv2D(64, kernel_size=(5, 5), activation='relu', padding='valid', name='C3'))\n    model.add(BatchNormalization())\n    # S4 Sub-sampling layer (pooling layer)\n    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), name='S4'))\n\n    # Flatten the feature maps\n    model.add(Flatten(name='Flatten'))\n\n    # FC6 Fully connected\n    model.add(Dense(120, activation='relu', name='FC6'))\n    model.add(BatchNormalization())\n\n    # FC7 Fully connected\n    model.add(Dense(84, activation='relu', name='FC7'))\n    model.add(BatchNormalization())\n\n    # FC8 output\n    model.add(Dense(5, activation='softmax', name='FC8'))\n\n    return model\n\n# Create the improved LeNet model\nimproved_lenet_model = Improved_LeNet()\n\n# Compile the model\nimproved_lenet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:20:38.481231Z","iopub.execute_input":"2024-04-02T02:20:38.481493Z","iopub.status.idle":"2024-04-02T02:20:38.633788Z","shell.execute_reply.started":"2024-04-02T02:20:38.481471Z","shell.execute_reply":"2024-04-02T02:20:38.632876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = train_model(improved_lenet_model, train_images, train_labels, valid_images, valid_labels, batch_size=32, epochs=200)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:20:38.635211Z","iopub.execute_input":"2024-04-02T02:20:38.635646Z","iopub.status.idle":"2024-04-02T02:21:25.041276Z","shell.execute_reply.started":"2024-04-02T02:20:38.635609Z","shell.execute_reply":"2024-04-02T02:21:25.040261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot evaluation\nplot_evaluation(history)\n\n# Evaluate on test set\ntest_loss_improved_lenet, test_accuracy_improved_lenet = improved_lenet_model.evaluate(test_images, test_labels)\nprint(\"Test Loss:\", test_loss_improved_lenet)\nprint(\"Test Accuracy:\", test_accuracy_improved_lenet)\n\n# Save the model\nimproved_lenet_model.save('/kaggle/working/lenet_model_v2.keras')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:21:25.042612Z","iopub.execute_input":"2024-04-02T02:21:25.04291Z","iopub.status.idle":"2024-04-02T02:21:27.64249Z","shell.execute_reply.started":"2024-04-02T02:21:25.042885Z","shell.execute_reply":"2024-04-02T02:21:27.641669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # ResNet18","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, ZeroPadding2D, Conv2D, BatchNormalization, Activation, MaxPooling2D, Add, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\n\ndef identity_block(X, filters):\n    F1, F2, F3 = filters\n\n    # Save the input value (shortcut path)\n    shortcut = X\n\n    # Main path\n    X = Conv2D(filters=F1, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters=F2, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters=F3, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n\n    # Add shortcut value to main path\n    X = Add()([X, shortcut])\n    X = Activation('relu')(X)\n\n    return X\n\ndef convolutional_block(X, filters, s=2):\n    F1, F2, F3 = filters\n\n    # Save the input value (shortcut path)\n    shortcut = X\n\n    # Convolutional shortcut path\n    shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid')(shortcut)\n    shortcut = BatchNormalization(axis=3)(shortcut)\n\n    # Main path\n    X = Conv2D(filters=F1, kernel_size=(3, 3), strides=(s, s), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters=F2, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters=F3, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n\n    # Add shortcut value to main path\n    X = Add()([X, shortcut])\n    X = Activation('relu')(X)\n\n    return X\n\ndef ResNet18(input_shape=(224, 224, 3), num_classes=5):\n    input = Input(input_shape)\n\n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(input)\n\n    # Stage 1\n    X = Conv2D(64, (7, 7), strides=(2, 2))(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n    # Stage 2\n    X = convolutional_block(X, filters=[64, 64, 64], s=1)\n    X = identity_block(X, filters=[64, 64, 64])\n\n    # Stage 3\n    X = convolutional_block(X, filters=[128, 128, 128], s=2)\n    X = identity_block(X, filters=[128, 128, 128])\n\n    # Stage 4\n    X = convolutional_block(X, filters=[256, 256, 256], s=2)\n    X = identity_block(X, filters=[256, 256, 256])\n\n    # Stage 5\n    X = convolutional_block(X, filters=[512, 512, 512], s=2)\n    X = identity_block(X, filters=[512, 512, 512])\n\n    # Average pooling\n    X = GlobalAveragePooling2D()(X)\n\n    # Output layer\n    X = Dense(num_classes, activation='softmax')(X)\n\n    # Create model\n    model = Model(inputs=input, outputs=X)\n    return model\n\nif __name__ == '__main__':\n    model_resnet18 = ResNet18()\n    print(model_resnet18.summary())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:21:27.647619Z","iopub.execute_input":"2024-04-02T02:21:27.647931Z","iopub.status.idle":"2024-04-02T02:21:28.268314Z","shell.execute_reply.started":"2024-04-02T02:21:27.647906Z","shell.execute_reply":"2024-04-02T02:21:28.267474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_resnet18.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_resnet18.fit(train_images, train_labels, batch_size=32, epochs=10, validation_data=(valid_images, valid_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:21:28.269398Z","iopub.execute_input":"2024-04-02T02:21:28.269693Z","iopub.status.idle":"2024-04-02T02:24:36.948014Z","shell.execute_reply.started":"2024-04-02T02:21:28.269669Z","shell.execute_reply":"2024-04-02T02:24:36.947163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Biểu đồ độ chính xác\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Biểu đồ mất mát\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:24:36.949709Z","iopub.execute_input":"2024-04-02T02:24:36.949973Z","iopub.status.idle":"2024-04-02T02:24:37.342663Z","shell.execute_reply.started":"2024-04-02T02:24:36.94995Z","shell.execute_reply":"2024-04-02T02:24:37.341759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss_resnet18, test_accuracy_resnet18  = model_resnet18.evaluate(test_images, test_labels)\nprint('Test Loss:', test_loss_resnet18)\nprint('Test Accuracy:', test_accuracy_resnet18)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:24:37.343908Z","iopub.execute_input":"2024-04-02T02:24:37.344359Z","iopub.status.idle":"2024-04-02T02:24:40.253411Z","shell.execute_reply.started":"2024-04-02T02:24:37.344326Z","shell.execute_reply":"2024-04-02T02:24:40.252512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lưu mô hình vào một tệp tin\nmodel_resnet18.save('/kaggle/working/resnet18_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:24:40.254628Z","iopub.execute_input":"2024-04-02T02:24:40.255255Z","iopub.status.idle":"2024-04-02T02:24:40.788517Z","shell.execute_reply.started":"2024-04-02T02:24:40.255221Z","shell.execute_reply":"2024-04-02T02:24:40.787647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, ZeroPadding2D, Conv2D, BatchNormalization, Activation, MaxPooling2D, Add, GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\n\ndef identity_block(X, filters):\n    F1, F2, F3 = filters\n\n    # Save the input value (shortcut path)\n    shortcut = X\n\n    # Main path\n    X = Conv2D(filters=F1, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters=F2, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters=F3, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n\n    # Add shortcut value to main path\n    X = Add()([X, shortcut])\n    X = Activation('relu')(X)\n\n    return X\n\ndef convolutional_block(X, filters, s=2):\n    F1, F2, F3 = filters\n\n    # Save the input value (shortcut path)\n    shortcut = X\n\n    # Convolutional shortcut path\n    shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid')(shortcut)\n    shortcut = BatchNormalization(axis=3)(shortcut)\n\n    # Main path\n    X = Conv2D(filters=F1, kernel_size=(3, 3), strides=(s, s), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters=F2, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n\n    X = Conv2D(filters=F3, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n    X = BatchNormalization(axis=3)(X)\n\n    # Add shortcut value to main path\n    X = Add()([X, shortcut])\n    X = Activation('relu')(X)\n\n    return X\n\ndef ResNet18_deep(input_shape=(224, 224, 3), num_classes=5):\n    input = Input(input_shape)\n\n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(input)\n\n    # Stage 1\n    X = Conv2D(64, (7, 7), strides=(2, 2))(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n    # Stage 2\n    X = convolutional_block(X, filters=[64, 64, 64], s=1)\n    X = identity_block(X, filters=[64, 64, 64])\n    X = identity_block(X, filters=[64, 64, 64])\n\n    # Stage 3\n    X = convolutional_block(X, filters=[128, 128, 128], s=2)\n    X = identity_block(X, filters=[128, 128, 128])\n    X = identity_block(X, filters=[128, 128, 128])\n\n    # Stage 4\n    X = convolutional_block(X, filters=[256, 256, 256], s=2)\n    X = identity_block(X, filters=[256, 256, 256])\n    X = identity_block(X, filters=[256, 256, 256])\n    \n    # Stage 5\n    X = convolutional_block(X, filters=[512, 512, 512], s=2)\n    X = identity_block(X, filters=[512, 512, 512])\n    X = identity_block(X, filters=[512, 512, 512])\n\n    # Average pooling\n    X = GlobalAveragePooling2D()(X)\n\n    # Output layer\n    X = Dense(num_classes, activation='softmax')(X)\n\n    # Create model\n    model = Model(inputs=input, outputs=X)\n    return model\n\nif __name__ == '__main__':\n    model_resnet18_deep = ResNet18_deep()\n    print(model_resnet18_deep.summary())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:24:40.789929Z","iopub.execute_input":"2024-04-02T02:24:40.790381Z","iopub.status.idle":"2024-04-02T02:24:41.638318Z","shell.execute_reply.started":"2024-04-02T02:24:40.790346Z","shell.execute_reply":"2024-04-02T02:24:41.637357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\n\n# Compile model\nmodel_resnet18_deep.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = train_model(model_resnet18_deep, train_images, train_labels, valid_images, valid_labels, batch_size=32, epochs=200)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:24:41.639512Z","iopub.execute_input":"2024-04-02T02:24:41.639768Z","iopub.status.idle":"2024-04-02T02:28:52.081897Z","shell.execute_reply.started":"2024-04-02T02:24:41.639745Z","shell.execute_reply":"2024-04-02T02:28:52.080723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Evaluate the model on test set\ntest_loss_resnet18_deep, test_accuracy_resnet18_deep = model_resnet18_deep.evaluate(test_images, test_labels)\nprint(\"Test Loss:\", test_loss_resnet18_deep)\nprint(\"Test Accuracy:\", test_accuracy_resnet18_deep)\n\n# Plot evaluation metrics\nplot_evaluation(history)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:28:52.087109Z","iopub.execute_input":"2024-04-02T02:28:52.087401Z","iopub.status.idle":"2024-04-02T02:28:54.446861Z","shell.execute_reply.started":"2024-04-02T02:28:52.087377Z","shell.execute_reply":"2024-04-02T02:28:54.446035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model (version 2)\nmodel_resnet18_deep.save(\"resnet18_deep_model_v2.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:28:54.447917Z","iopub.execute_input":"2024-04-02T02:28:54.448194Z","iopub.status.idle":"2024-04-02T02:28:55.189391Z","shell.execute_reply.started":"2024-04-02T02:28:54.448171Z","shell.execute_reply":"2024-04-02T02:28:55.188519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Tạo DataFrame từ dictionary\ntest_metrics = {\n    \"Model\": [\"MLP\", \"Improved MLP\", \"Simple CNN\", \"Improved CNN\", \"VGG16\", \"LeNet\", \"Improved LeNet\", \"ResNet18\", \"ResNet18 Deep\"],\n    \"Test Loss\": [test_loss_mlp, test_loss_improved_mlp, test_loss_simple_cnn, test_loss_improved_cnn, test_loss_vgg16, test_loss_lenet, test_loss_improved_lenet, test_loss_resnet18, test_loss_resnet18_deep],\n    \"Test Accuracy (%)\": [test_accuracy_mlp * 100, test_accuracy_improved_mlp * 100, test_accuracy_simple_cnn * 100, test_accuracy_improved_cnn * 100, test_accuracy_vgg16 * 100, test_accuracy_lenet * 100, test_accuracy_improved_lenet * 100, test_accuracy_resnet18 * 100, test_accuracy_resnet18_deep * 100]\n}\n\ndf_test_results = pd.DataFrame(test_metrics)\nprint(df_test_results)\n# Lưu DataFrame vào file CSV\ndf_test_results.to_csv('/kaggle/working/model_evaluation.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T02:28:55.190734Z","iopub.execute_input":"2024-04-02T02:28:55.191118Z","iopub.status.idle":"2024-04-02T02:28:55.20736Z","shell.execute_reply.started":"2024-04-02T02:28:55.191068Z","shell.execute_reply":"2024-04-02T02:28:55.206423Z"},"trusted":true},"execution_count":null,"outputs":[]}]}